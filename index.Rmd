---
title: "Story Time!"
author: "morris"
date: "`r Sys.Date()`"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
---

```{r, include=FALSE}
# Idea: how happy is major? compare words for different key modes
# Idea: what is the most thought provoking key?
# Idea: do frequent modulations mean different lyrics?
#   (compute modulation score per track)
# Idea: how does timbre or number of sections influence lyrics
```

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(ggplot2)
library(compmus)
library(jsonlite)
library(dplyr)
library(cowplot)

# load env vars
readRenviron(".Renviron")
```

```{r, include=FALSE}
#! request lyrics from Musixmatch

LYRICS_APIKEY <- Sys.getenv("MUSIXMATCH_APIKEY")

LYRICS_URL <- sprintf("https://api.musixmatch.com/ws/1.1/matcher.lyrics.get?
q_track=%s&q_artist=%s&apikey=%s", "%s", "%s", LYRICS_APIKEY)
LYRICS_URL <- gsub("\n", "", LYRICS_URL)

# TODO: store lyrics in json file
#' retrieves the lyrics of track from artists
get_lyrics <- function(track, artists) {
  # there may be no spaces in the URL
  track <- gsub(" ", "%20", track)
  artists <- gsub(" ", "%20", artists)
  
  data <- jsonlite::fromJSON(sprintf(LYRICS_URL, track, artists))
  lyrics <- data$message$body$lyrics$lyrics_body
  # remove warning at the end
  lyrics <- gsub(
    "\n...\n\n\\*{7} This Lyrics is NOT for Commercial use \\*{7}",
    "", lyrics
  )
  return(lyrics)
}
```

```{r, include=FALSE}
# load data
alb_elephant <- get_playlist_audio_features("", "1PyDPL0yOSFInNZrCB9fX4")
alb_moneystore <- get_playlist_audio_features("", "5d1UtUJst5yqIrRrFyy14b")
alb_madvillain <- get_playlist_audio_features("", "5TtY9ZS9P8HWCFr1AvVx1p")
```

```{r, include=FALSE}
# data filter
process_alb <- function(df) {
  return (df %>%
          # select columns 
           select(danceability, energy, key, loudness, mode, speechiness, acousticness,
            instrumentalness, liveness, valence, tempo, time_signature, track.artists,
            track.duration_ms, track.name, track.popularity, track.album.name, track.track_number,
            key_name, mode_name, key_mode
          ) %>%
          # abbreviate track names
            mutate(track.short_name = paste(substring(track.name, 0, 10), "...", sep=""))
  )
}

# filter data
alb_elephant_feats <- process_alb(alb_elephant)
alb_moneystore_feats <- process_alb(alb_moneystore)
alb_madvillain_feats <- process_alb(alb_madvillain)

all_albs <- rbind(alb_elephant_feats, alb_moneystore_feats, alb_madvillain_feats)
```

```{r, include=FALSE}
#! some useful shared functionality

# TODO: why no work?
# returns the spectrogram of the given track
make_spectrogram <- function(track_analysis, segmentation, method="mean", norm="manhattan") {
  track_analysis %>%
    compmus_align(segmentation, segments) %>%
    select(segmentation) %>%
    unnest(segmentation) %>%
    mutate(pitches=map(
        segments, compmus_summarize, pitches,
        method=method, norm=norm
    ))
}

```

```{r, include=FALSE}
#! fetch lyrics

LYRICS_FILE <- "res/lyrics.json"

# load stored lyrics
if (!file.exists(LYRICS_FILE)) {
  lyrics_db <- data.frame(
    track=character(), artists=character(),
    album=character(), lyrics=character()
  )
} else {
  lyrics_db <- jsonlite::fromJSON(LYRICS_FILE)
}

# load all lyrics
by(all_albs, seq_len(nrow(all_albs)), function(row) {
  if (!(row$track.name %in% lyrics_db$track)) {
    track <- row$track.name
    # concatenate artist names
    artists <- paste(row$track.artists[[1]]$name, collapse=", ")
    # fetch lyrics
    lyrics <- get_lyrics(track, artists)
    
    # store lyrics  
    lyrics_db <<- lyrics_db %>% add_row(
      track=track, album=row$track.album.name, artists=artists,
      lyrics=lyrics
    )
  }
})

# store lyrics
write(jsonlite::toJSON(lyrics_db, pretty=TRUE), LYRICS_FILE)

head(lyrics_db)
```
### Key analysis

```{r}
#! generate chord templates

shift_template <- function(temp, n) {
  if (n == 0) temp
  else c(tail(temp, n), head(temp, -n))
}

keys <- c("C", "Db", "D", "Eb", "E", "F", "Gb", "G", "Ab", "A", "Bb", "B")
mode_templates <- list(
  # TODO: where are these from?
  maj=c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88),
  min=c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
)

key_templates <- list()
for (key_idx in seq_along(keys)) {
  for (mode in names(mode_templates)) {
    chord_name <- sprintf("%s:%s", keys[key_idx], mode)
    template <- shift_template(mode_templates[[mode]], key_idx-1)
    
    key_templates[[chord_name]] <- template
  }
}

key_templates <- tibble(name=names(key_templates), template=key_templates)
```

```{r, include=FALSE}
# smooth sailing
ss_analysis <- get_tidy_audio_analysis("5fb0p81SAZBkZEvmLAfGKz") %>%
  compmus_align(sections, segments) %>%
    select(sections) %>%
    unnest(sections) %>%
    mutate(pitches=map(
        segments, compmus_summarize, pitches,
        method="mean", norm="manhattan"
    ))
```

```{r, include=FALSE}
# measure how well each key matches for each section
ss_keys <- ss_analysis %>%
  compmus_match_pitch_template(
    key_templates,
    method="chebyshev",
    norm="manhattan"
  ) %>%
  mutate(d = 1 - d)

# find the changes
# TODO
```

```{r, include=FALSE}
ss_keys %>%
  group_by(start) %>%
  top_n(n=3, wt=d)
```

```{r}
color_palette <- "mako" # "cividis" is fun also

ss_keygram_plot <- ss_keys %>%
  ggplot(aes(start + duration/2, name, width=duration, fill=d)) +
  geom_tile() +
  scale_fill_viridis_c(option=color_palette, guide="none") +
  theme_bw() +
  labs(x="time (s)", y="", title="Key matching for Smooth Sailing") +
  geom_vline(xintercept=130, colour = "purple") +
  geom_vline(xintercept=215, colour = "yellow")


ss_loudness_plot <- ss_analysis %>%
  ggplot(aes(start + duration/2, loudness)) +
  geom_line() +
  theme_bw() +
  labs(x="time (s)", y="loudness (db)", title="Loudness for Smooth Sailing") +
  geom_vline(xintercept=130, colour = "purple") +
  geom_vline(xintercept=215, colour = "yellow")

plot_grid(ss_keygram_plot, ss_loudness_plot)

```

***

Let's analyze keys. To accomplish this, we will match the pitch classes
we expect to hear in a key, with the actually played pitch classes.
Let's do this for an archetypal catchy rock tune: Smooth Sailing, from
the Queens of the Stone Age. When we do this for all minor and major keys,
we end up with the first plot. The brighter the tile, the more prevalent
the key on the y-axis is. You will notice that at two points, near
135 and 215 seconds, many different keys are matched at once. When we
look at the loudness plot these points coincide with the peaks in loudness.
This makes sense: these points in the songs are mostly pitch-class-less noise.


### Keys histogram

```{r}
all_albs %>%
  ggplot(aes(key_name, fill=mode_name)) +
  geom_dotplot() +
  labs(title="Key occurences in data set", x="Key", y="Occurences", fill="Mode")
```

***

Here we see which keys occur how many times in the data set. Funilly, we
some keys seem to be chiefly minor, like B and E, and some major, like G or D.
This could be a limitation of the Spotify API, namely that it confuses
relative minor and minor keys, like E minor and G major.

### Introduction

This period I would like to investigate how song lyrics correlate with music properties such as modality, energy levels and dynamic pitch range across different genres. More specifically, the research focuses on relating the words contained by the lyrics to musical properties. I find it highly interesting to see how the mood and emotionality of a song/genre affect a sing and songwriter when writing the lyrics. I expect some obvious results, such that hip hop is generally more about the ‘hood’ than rock music, and perhaps that songs in a minor mode deal with sad topics more frequently than major mode songs. 

 

Because the lyrical vocabulaire is extremely rich, a large, diverse dataset is of the essence. To accomplish this and to keep the corpus representative, I will put together a corpus that draws inspiration from a broad range of genres. This includes mainstream genres such as pop music, but also more obscure ones such as industrial hip hop, as unexpected yet interesting patterns may emerge. I will dissect the word usage for each genre and then compare word usages of different genres. It would be interesting to know if genres with similar lyrics have similar properties.

 

A list of albums per genre that make up the corpus (for now):

*Pop*: 
Midnights (Taylor Swift); WHEN WE ALL FALL ASLEEP, WHERE DO WE GO? (Billie Eilish); Dua Lipa (Dua Lipa)
 
*Hip Hop*:
Madvillainy (MF DOOM, Madlib); ASTROWORLD (Travis Scott); HEROES & VILLAINS (Metro Boomin)

*Alternative Rock*:
Elephant (The White Stripes); ..Like Clockwork (Queens of the Stone Age); Street Worms (Viagra Boys)

*Industrial hip hop*:
The Money Store (Death Grips); OFFLINE! (JPEGMAFIA); Visions of Bodies Being Burned (clipping.)

*Classical (translated to english)*:
Wilhelmus (Marnix Van St. Aldegonde), Negende symfonie (Beethoven)


### Analysis of an odd one out
```{r, include=FALSE}

sna_analysis <- get_tidy_audio_analysis("3dPQuX8Gs42Y7b454ybpMR")

sna_bar <- sna_analysis %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(pitches=map(segments, compmus_summarise, pitches,
        method="rms", norm="euclidean"
  )) %>%
  mutate(timbre=map(segments, compmus_summarise, timbre,
        method="rms", norm="euclidean"
  ))

sna_beat <- sna_analysis %>%
  compmus_align(beats, segments) %>%
  select(beats) %>%
  unnest(beats) %>%
  mutate(pitches=map(segments, compmus_summarize, pitches,
        method="rms", norm="euclidean"
  )) %>%
  mutate(timbre=map(segments, compmus_summarize, timbre,
        method="rms", norm="euclidean"
  ))

```

```{r}
sna_beat %>%
  compmus_gather_timbre() |>
  ggplot(aes(
      start + duration/2, basis,
      width=duration,
      fill=value
  )) +
  geom_tile() +
  labs(x="time (s)", y=NULL, fill="magnitude") +
  scale_fill_viridis_c() +                              
  theme_minimal()
```

***
In order for us to broaden our understanding of the entire dataset, it can
be useful zoom in and investigate a single track. Let us investigate not
any song, but one of the most iconic out of the entire dataset: Seven Nation
Army.
To the side, you will find a cepstrogram, with a level of detail per beat. As 
you can see, the timbre varies between two of the timbre properties. After
listening to the song this makes sense: the electric guitar amplifier
settings change between the chorus and verse. Though, the image is a bit
noisy, so let's investigate a more granular time division.

### Analysis of an odd one out, a bit less specific

```{r}
sna_bar %>%
  compmus_gather_timbre() |>
  ggplot(aes(
      start + duration/2, basis,
      width=duration,
      fill=value
  )) +
  geom_tile() +
  labs(x="time (s)", y=NULL, fill="magnitude") +
  scale_fill_viridis_c() +                              
  theme_minimal()
```

***
Now the time division is per bar. This gives a clearer image and it
emphasizes our conclusion.

### Song structure analysis

```{r}
sna_bar %>%
  compmus_self_similarity(timbre, "cosine") %>%
  ggplot(aes(
      xstart + xduration/2,
      ystart + yduration/2,
      width=xduration,
      height=yduration,
      fill=d
  )) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide="none") +
  theme_minimal() +
  labs(x="", y="")
```

***
Let us try to delve deeper into our hypothesis that the different
timbres are caused by differences in chorus and verse.
To accomplish this, we will generate a self-similarity
matrix for timbre and pitch.
Here you see the self-similarity matrix of seven nation army for timbre.
Our conclusion is made even stronger, it seems the timbre is split into
a two parts, possibly.

### Song structure analysis

```{r}
sna_bar %>%
  compmus_self_similarity(pitches, "cosine") %>%
  ggplot(aes(
      xstart + xduration/2,
      ystart + yduration/2,
      width=xduration,
      height=yduration,
      fill=d
  )) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide="none") +
  theme_minimal() +
  labs(x="", y="")
```

***
To be certain here I present the pitch self-similarity matrix also.
This also strongens our evidence that the song is split into two parts,
we can now say this with full certainty.
The blocky pattern can be explained by the chord changes.

### Comparison of tracks within an album

```{r}
# plot
ggplot(alb_elephant_feats, aes(track.short_name, energy, fill="red")) +
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1)) +
  labs(title="Energy levels for tracks on 'Elephant' by The White Stripes") +
  geom_col()
```

***

Before we shall discover inter-album relations, let us commit to a single album,
such that we can shape an idea what the variation within an album might look 
like.


### Comparison of different albums

```{r}
ggplot(all_albs, aes(tempo, energy, col=track.album.name, size=track.popularity)) +
  labs(title="Energy vs. Tempo") +
  geom_point()
```

***

### Self-similarity

Intuitively, tempo and energy are correlating factors of a song. We imagine
high energy songs generally have a fast tempo. Let us investigate this
thought by plotting the data for number of albums. As you can see, some
albums tend to be limited in their energy range, whilst others are
more or less contained.

### Conclusion

What is there not to say. We live in a data driven society that harbors
as many music tastes as there are colors in a van Gogh painting. But just
like a van Gogh painting, you can dissect it and scrutinize the most 
elementary aspects, from its radiance to its perspective on the cruelties and
absurdities of society. We looked at energy levels, at tempo, a variety
of tracks and albums, from a point of view of strict objectiveness, one
that our primordial ancestors would not even be able to fathom. One could
draw an infinite number of conclusions, some might jump out more than others.
What is a data driven society without conclusions?


