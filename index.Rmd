---
title: "Portfolio"
author: "Morris de Haan"
date: "`r Sys.Date()`"
output:
  flexdashboard::flex_dashboard:
    storyboard: true
    
    css: style.css
    theme:
      version: 4
      bootswatch: cerulean
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(ggplot2)
library(compmus)
library(jsonlite)
library(dplyr)
library(plotly)
library(cowplot)
library(showtext)
library(tm)
library(wordcloud2)
library(patchwork)
library(caret)
library(recipes)
library(tidymodels)
library(doParallel)
library(vip)

# load environment variables
readRenviron(".Renviron")

portfolio_font <- "Times New Roman"
custom_theme <- theme(text=element_text(family=portfolio_font))

RUN_XGBOOST_TUNING = TRUE
```

```{r load corpus, include=FALSE}
#! fetch data

corpus_raw <- get_playlist_audio_features("", "1H9BY5uJqk2CWtPo4Ogi2w")
```

```{r preprocess corpus, include=FALSE}
#! preprocess data

corpus <- corpus_raw %>% select(
  acousticness, danceability, energy, instrumentalness, loudness, mode, tempo, valence, key,
  popularity=track.popularity, artists=track.artists, track=track.name
)

# key index --> key name
pretty_keys <- c("C", "C♯|D♭", "D", "D♯|E♭", "E", "F", "F♯|G♭", "G", "G♯|A♭",  "A", "A♯|B♭", "B")
ugly_keys <- c("C", "C#|Db", "D", "D#|Eb", "E", "F", "F#|Gb", "G", "G#|Ab", "A", "A#|Bb", "B")
corpus$key <- unlist(map(corpus$key, function(idx) if (idx == -1) "Unknown" else pretty_keys[idx+1]))

# mode index --> mode name
corpus$mode <- unlist(map(corpus$mode, function(idx) if (idx == 1) "Major" else "Minor"))

# concatenate artist names into single string
corpus$artists <- unlist(map(corpus$artists, function(r) paste(r$name, collapse=", ")))
```

```{r fetch lyrics, include=FALSE}
#! load lyrics, delete entries from corpus whose lyrics cannot be fetched

LYRICS_DIR <- "./res"
LYRICS_FILE <- "./res/lyrics.json"
LYRICS_APIKEY <- Sys.getenv("MUSIXMATCH_APIKEY")
LYRICS_URL <- sprintf(paste(
  "https://api.musixmatch.com/ws/1.1/matcher.lyrics.get?",
  "q_track=%s&q_artist=%s&apikey=%s", sep=""),
  "%s", "%s", LYRICS_APIKEY
)

# fetches the lyrics of a track from some artists from Musixmatch
get_lyrics <- function(track, artists) {
  # there may be no spaces in the URL
  track <- gsub(" ", "%20", track)
  artists <- gsub(" ", "%20", artists)
  
  data <- jsonlite::fromJSON(sprintf(LYRICS_URL, track, artists))
  lyrics <- data$message$body$lyrics$lyrics_body
  # remove warning at the end
  lyrics <- gsub(
    "\n...\n\n\\*{7} This Lyrics is NOT for Commercial use \\*{7}.*",
    "", lyrics
  )
  return(lyrics)
}

# load stored lyrics
if (!file.exists(LYRICS_FILE)) {
  lyrics_db <- data.frame(
    track=character(), artists=character(),
    album=character(), lyrics=character()
  )
  
  dir.create(LYRICS_DIR)
} else {
  # fetch all previously cached lyrics
  lyrics_db <- jsonlite::fromJSON(LYRICS_FILE)
}

for (i in 1:nrow(corpus)) {
  row <- corpus[i,]
  
  # check if lyrics are already loaded
  if (!(row$track %in% lyrics_db$track)) {
    # fetch lyrics
    lyrics <- get_lyrics(row$track, row$artists)
    if (length(lyrics) == 0)
      lyrics <- NA
    
    # store lyrics
    lyrics_db <- lyrics_db %>% add_row(
      track=row$track, artists=row$artists,
      lyrics=lyrics
    )
  }
}

# cache lyrics
write(jsonlite::toJSON(lyrics_db, pretty=TRUE), LYRICS_FILE)
```

```{r compute sentiment, include=FALSE}
#! add lyrical sentiment values to corpus, delete tracks from corpus
#!  that have no available lyrics

# compute sentiments
system("python3 lyrics.py")

lyrics_db <- jsonlite::fromJSON(LYRICS_FILE)[c("track", "sentiment")]

corpus <- merge(corpus, lyrics_db, by="track") %>%
  rename(lyrical_sentiment=sentiment) %>%
  drop_na(lyrical_sentiment)
```

### Introduction

How does music relate to lyrics? It is tempting to think that a song tries to convey some feeling or emotion, and that both the music and lyrics are there to support this message. Let me give you an example. We might expect a song with a slow beat and laid back guitar to talk about laid back topics, maybe a trip to the beach. At the other end of the spectrum, heavy metal would likely concern itself with darker, heavier subjects. However, are these suspicions even true? Let’s put some numbers to the hypothesis that there in fact is a relationship between music and lyrics. In the next sections I’ll take you through a journey where we approach this topic with a statistical mindset, harnessing all the powers that modern technology has to offer along the way.

We'll start out with picking a large body of music and for each track in there we are going to collect the lyrics. It would be way too cumbersome to scrape all the lyrics from the internet manually, but fortunately the [`Musixmatch`](https://www.musixmatch.com/) API
makes it possible to query lyrics from code in a single API call. For an unpaid account only 30% of the lyrics for a queried track is returned, but that will do for our intents and purposes. The lyrics themselves are not enough however, we need to somehow capture meaning in a numerical value in order to run statistical tests on them. Therefore I assign every set of lyrics a sentimental, also known as valency, score automatically computed by the [`NLTK`](https://www.nltk.org/) package, which offers natural language processing functionalities. A low score indicates a *sad* sentiment/feeling, whereas a high score a *happy* sentiment/feeling. To be precise, the score ranges from `-1.0` to `1.0`. This arms us with the capability to analyze lyrics' mood, which is arguably one of the most important aspects of music. The more topic oriented side of lyrics analysis concerning the actual intellectual 'meaning' of words we'll leave aside in this research. Equipped with a strategy on quantifying numbers, we can start to tackle the problem of how to approach computational music analysis.

Instead of zeroing in on a particular aspect of music or a niche genre or artist, we are going to keep the research broad. We'll focus on music as a whole so we can draw interesting conclusions about music as a whole. Throughout this storyboard we'll divvy up music into four main elements: *melody*, *harmony*, *instrumentation* and *rhythm*. For each of these we'll attempt to confirm or deny hypotheses that are grounded in intuition, but not (yet) in science. Of course we still require a way to quantify music, but luckily [`Spotify`](https://developer.spotify.com/documentation/web-api) is kind enough to provide a goldmine of musical data that we'll use.

Let us dive into it!

<br><br><br><br>
Valencies for two hypothetical sets of lyrics:

`What a sad, miserable day. It's raining and I have debts to pay!` → <span style="color: red">`-0.79`</span>

`It's the most jolly moment in a while, to treat my granny to a smile!` → <span style="color: green">`0.84`</span>

### Corpus

The first order of business is choosing the corpus of music. We have chosen a broad research question and the corpus should
reflect that. Meaning it should draw inspiration from a large variety and number of songs, only then can we
justify general conclusions. The reason I won't simply pick the top-50 songs for each decade, or any top list for that matter, is because such a list would be heavily pop dominated. There is much more to music than just popular music. What is interesting is emphasizing *variety* as opposed to *the most popular*, therefore we shall cover a range of different genres. To be clear, this distinction does not exclude the most popular songs. All songs in the corpus are sourced from albums that are exemplary of the niche they occupy and popular within their genre, only not necessarily *the* most popular of their time. The reason I will proceed with an album oriented approach instead of including all songs from a number of artists or top list of each genre, is in the case of the former that some artists have produced hundreds, some only a dozen of songs which would imbalance the data set, and in the case of the latter the problem I discussed before (stressing *variety* instead of *the most popular*).

To summarize, we want a large corpus from a variety of genres, which, because the heavy lifting in terms of fetching the data is done by the `Musixmatch` and `Spotify` APIs, is most certainly possible. The final list of albums that are included in this research:

* <span style="color: #FF6F61">Pop</span>
WHEN WE ALL FALL ASLEEP, WHERE DO WE GO? *[Billie Eilish]*, 
Dua Lipa *[Dua Lipa]*, 
Midnights *[Taylor Swift]*, 
Thriller *[Michael Jackson]*

* <span style="color: #6495ED">Classic Rock</span>
The White Album *[The Beatles]*, 
Rumours *[Fleetwood Mac]*

* <span style="color: #C7A804">Hip Hop</span>
Madvillainy *[Madvillain]*, 
HEROES & VILLAINS *[Metro Boomin]*

* <span style="color: #877A7A">Industrial</span>
The Money Store *[Death Grips]*, 
OFFLINE! *[JPEGMAFIA]*

* <span style="color: #28A628">Alternative</span>\*
Elephant *[The White Stripes]*, 
Street Worms *[Viagra Boys]*, 
..Like Clockwork *[Queens of the Stone Age]*, 
In the Aeroplane Over the Sea *[Neutral Milk Hotel]*, 
OK Computer *[Radiohead]*, 
Hawaii: Part II *[Miracle Musical]*

* <span style="color: #808080">Miscellaneous</span>†
Demon Days *[Gorillaz]*, 
Plastic Beach *[Gorillaz]*, 
St. Elsewhere *[Gnarls Barkley]*


This totals **284** tracks and **16** hours of listening time.

<br><br><br><br>
\* You might've noticed a lot of albums have been selected for this genre, but the alternative genre is extremely broad and the albums are highly diverse.

† By Miscellaneous I simply denote albums that cross the barriers of genre as they contain
numerous elements of a plethora of genres.

<br>
*Disclaimer: Please note that these genre classifications are not objective, but subjective and approximate.*

***

**Playlist**

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/1H9BY5uJqk2CWtPo4Ogi2w?utm_source=generator&theme=0" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" data-external=1 loading="lazy"></iframe>

### Discovery

```{r discovery}
ggplotly(
  ggplot(corpus, aes(valence, lyrical_sentiment, col=loudness, size=popularity, text=paste("track:", track, "by", artists))) +
  geom_point() +
  geom_smooth(inherit.aes=FALSE, aes(valence, lyrical_sentiment)) +
  geom_rug(linewidth=0.2) +
  labs(
    title="Musical vs. Lyrical Valence, where size denotes popularity", x="Musical valence", y="Lyrical Valence",
    col="Loudness (decibels)", size="Popularity"
  ) +
  custom_theme
)
```

***

*Research: What are the general, high level patterns in the data?*

Before we look at the individual musical elements as discussed earlier, we should try to explore the data we're dealing with. The `Spotify` API offers a plethora of functionalities that range from very high to very low level. Here we will use some the the high level analyses like *musical valence* and *loudness* to learn about the corpus. In particular, it makes most sense to compare the high level musical valence feature with our computed *lyrical valence* value and see if there is any connection.

When we do this we find something enormously interesting. Clearly, there is a correlation between musical and lyrical valence, but not in the way one would expect. Tracks with low musical valence **and** tracks with high musical valence correlate with high lyrical valence, whereas musical valences the middle of the pack point to lower lyrical valence. Due to the big size of the corpus this claim carries a lot of weight. Upon closer inspection a limitation of lyrical valence becomes apparent as well, namely the `NLTK` API struggles with lyrics that require a deeper understanding of (cultural) context and nuance. For example, [*bury a friend*](https://www.musixmatch.com/lyrics/Robert-14/Bury-a-Friend) by Billie Eilish has a lyrical valence of `0.94`, despite the lyrics clearly showcasing a low lyrical valency. This only an exception though, for the large majority of songs the lyrical valence makes a lot of sense, for example a high score (`0.99`) for [*Feel Good Inc.*](https://www.musixmatch.com/lyrics/Gorillaz/Feel-Good-Inc) by Gorillaz and a low score (`-0.94`) for [*Anti-Hero*](https://www.musixmatch.com/lyrics/Taylor-Swift/Anti-Hero) by Taylor Swift. This gives credence to the claim we made.

Let's try to dive even deeper and analyze a much more low level feature of music: melody.

### Melody

```{r melody}
get_audio_analysis <- function(uri) {
  get_tidy_audio_analysis(uri) %>%
    select(segments) %>%
    unnest(segments) %>%
    select(start, duration, pitches)
}

get_chromogram <- function(analysis, title) {
  analysis %>%
    mutate(pitches=map(pitches, compmus_normalize, "euclidean")) %>%
    compmus_gather_chroma() %>%
    ggplot(aes(start + duration*0.5, pitch_class, width=duration, fill=value)) +
    geom_tile() +
    labs(title=title, x="Time (seconds)", y=NULL, fill="Magnitude") +
    scale_fill_viridis_c(option="mako", guide="none") +
    theme_minimal() +
    custom_theme
}

# Ball and Biscuit by the white stripes
happy_analysis <- get_audio_analysis("0O2SYh5AZ0y8MAPOVC4Mxz")
# The Illest Villains by Madvillain
sad_analysis <- get_audio_analysis("2Jn0wHQ2lEif2gLRsyfaf2")

happy_entry <- (corpus %>% filter(track == "Ball and Biscuit"))
sad_entry <- (corpus %>% filter(track == "The Illest Villains"))

plot_grid(
  get_chromogram(happy_analysis,
    sprintf("%s chromogram (lyrical valency = %s)", happy_entry$track,
      format(happy_entry$lyrical_sentiment, digits=2)
  )),
  get_chromogram(sad_analysis,
    sprintf("%s chromogram (lyrical valency = %s)", sad_entry$track,
      format(sad_entry$lyrical_sentiment, digits=2)
  )),
  nrow=2
)
```

***

*Research: can we identify melodies from audio data?*

Intuitively, it seems that melody encodes a lot of the valency information of a song. The melody is usually the most
memorable part and often indicative of the feel of a song. So it makes sense to look at the melody of two tracks, one with low and one
with high lyrical valence, and investigate how melody correlates with lyrical valence. A sensible visualization tool to use is a chromogram. This captures for each moment the notes that are played, as analyzed using the fourier transform. Let's try this and see if any melody lines become apparent.

Unfortunately, looking at the chromograms, no discernible melody is recognizable. The only thing that sticks out is the droning 'E' in Ball and Biscuit, but this could hardly be called a melody. If you listen to the tracks there are clear, repeating melodies present (at least to the human ear) which on first thought should give rise to repeating patterns in the chromogram. Nothing could be less true. It appears we need a different tool.

### Melody/Harmony

```{r per key lyrical valence, message=FALSE}
per_key_valences <- rbind(
  # add average per key
  corpus %>%
    mutate(mode="Either") %>%
    group_by(key, mode) %>%
    summarize(avg_sentiment = mean(lyrical_sentiment), count = n()),
  corpus %>%
    group_by(key, mode) %>%
    summarize(avg_sentiment = mean(lyrical_sentiment), count = n()),
  # add average for all keys
  corpus %>%
    mutate(key="Mean\nof all\nkeys", mode="Either") %>%
    group_by(key, mode) %>%
    summarize(avg_sentiment = mean(lyrical_sentiment), count = n()),
  corpus %>%
    mutate(key="Mean\nof all\nkeys") %>%
    group_by(key, mode) %>%
    summarize(avg_sentiment = mean(lyrical_sentiment), count = n())
)

# get the max number of samples for any key
col_scale_end <- max((per_key_valences %>% filter(key != "Mean\nof all\nkeys"))$count)

bar_plot <- per_key_valences %>%
  ggplot(aes(key, avg_sentiment, fill=mode, col=count)) +
  geom_col(position=position_dodge(width=0.75)) +
  geom_vline(xintercept=12.5, linetype="dotted", col="black") +
  labs(title="Mean lyrical valence per key, for either, the major and the minor mode", x="Key", y="Mean lyrical valence", col="Count", fill="& Mode") +
  scale_fill_manual(values=c("Either"="gray", "Major"="lightblue", "Minor"="navy")) +
  scale_color_viridis_c(option="rocket", limits=c(0, col_scale_end)) +
  custom_theme

ggplotly(bar_plot)
```

***

*Research: what is the saddest key?*

Apparently it's difficult to find melodies when faced with a chromogram. Instead of identifying specific melody lines, we could
focus on the key in which the melody is played. Luckily `Spotify` gives us the key and mode of every track in our corpus, so
we don't have to compute this ourselves. When we plot the lyrical valency for each key, where the band around the bars represents
the number of tracks that have that specific key, we get the bar plot to the side. What meets the eye, is a huge spike
at the D sharp (or E flat) key. What could this mean? Unfortunately not a lot, because upon closer inspection it appears that
songs in that key are heavily underrepresented in the corpus. There does seem to be quite a bit of variation among keys,
especially the keys in B, which appear to affect lyrics in a negative way. This points to the fact that there in fact is such a thing, like "the saddest key" (which would be B major).

Although this could be coincidental and the effect may be canceled out if the corpus were much larger. Somethings that favor this conclusion
are the average lyrical valencies, which converge to a lyrical valency of `~0.13`. There is no significant distinction between the average major and minor mode, even though we're always told that minor keys are "sad" and major keys are "happy". These results deny what our music teachers have been telling us for centuries (at least, lyrically speaking)!

### Harmony detection limitations

```{r generate chord templates, include=FALSE}
#! generate chord templates

shift_template <- function(temp, n) {
  if (n == 0) temp
  else c(tail(temp, n), head(temp, -n))
}

mode_templates <- list(
  maj=c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88),
  min=c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
)

key_templates <- list()
for (key_idx in seq_along(ugly_keys)) {
  for (mode in names(mode_templates)) {
    chord_name <- sprintf("%s:%s", ugly_keys[key_idx], mode)
    template <- shift_template(mode_templates[[mode]], key_idx-1)
    
    key_templates[[chord_name]] <- template
  }
}

key_templates <- tibble(name=names(key_templates), template=key_templates)
```

```{r harmony, message=FALSE}
get_key_match_plot <- function(uri, title) {
  key_analysis <- get_tidy_audio_analysis(uri) %>%
    compmus_align(sections, segments) %>%
      select(sections) %>%
      unnest(sections) %>%
      mutate(pitches=map(
          segments, compmus_summarize, pitches,
          method="rms", norm="euclidean"
      ))
  
  key_matching <- key_analysis %>%
    compmus_match_pitch_template(
      key_templates,
      method="euclidean",
      norm="euclidean"
    ) %>%
    # invert colors
    mutate(d = 1 - d)
  
  key_matching %>%
    ggplot(aes(start + duration/2, name, width=duration, fill=d)) +
    geom_tile() +
    scale_fill_viridis_c(option="mako", guide="none") +
    labs(x="Time (seconds)", y="", title=title) +
    theme_minimal() +
    custom_theme
}

plot_grid(
  get_key_match_plot("0gTRROuntlrPQ64W3J2Etv", "Keygram for\nElectioneering (Radiohead)"),
  get_key_match_plot("0ubyD2iLcA6W90tMaKyXs2", "Keygram for\nUNTITLED (JPEGMAFIA)")
)
```

***

I should address some issues with automatic key matching that explain why we should take the idea that B major is the
saddest key with an even larger grain of salt. Key matching might work for a lot of tracks, but there are many cases where it fails too.
Where key matching fails most spectacularly, is for highly percussive tracks, which is usually the case for hip hop. This is due to
the inharmonic nature of most percussive instruments.
Take for instance *UNTITLED* by *JPEGMAFIA*. Upon listening, the energetic hi hats and fast drum kicks stand out.
This is manifested in the corresponding keygram, where for each section every key on the y-axis is matched.
The brighter the tile, the more strongly the key matched. We'd expect a straight line that changes height after a modulation. But in the *UNTITLED* track there is no such pattern to be found.

Another issue is brought forward by a limitation by the `Spotify` API, that is, only two unique modes can be distinguished by
the API (the major and minor mode). This is problematic, because many artists apply many different modes to achieve a variety
of effects that cannot be achieved by just minor or major keys. The song *Electioneering* by *Radiohead* is in D-dorian,
which is the minor key with a raised sixth. The result is that the key lies somewhere in between D-minor and D-major, which is reflected in the keygram.

Though we could match the keys ourselves for every track in the corpus, another issue would present. To match every possible mode,
the search space would become too big and our results too cluttered to identify a specific key, as multiple keys would always
match somewhat.

With that researched, we shall continue to investigate instrumentation.

### Instrumentation

```{r load instrumentation analysis}
get_timbre_analysis <- function(uri) {
  get_tidy_audio_analysis(uri) %>%
  compmus_align(beats, segments) %>%
  select(beats) %>%
  unnest(beats) %>%
  mutate(timbre=map(segments, compmus_summarize, timbre, method="rms", norm="euclidean"))
}

# Everybody's Got Something To Hide Except Me And My Monkey by The Beatles
valent_track <- get_timbre_analysis("64P3zpRsDHIk7YTpRtaKYL")
# I'm So Tired by The Beatles
unvalent_track <- get_timbre_analysis("2X9H5BokS1u5O46YpNYNsZ")

```

```{r plot instrumentation analysis, warning=FALSE}
LINE_COLS <- c("red", "green", "blue")
LINE_SIZE <- 2

get_timbre_plot <- function(analysis, title, caption) {
  analysis %>%
  compmus_self_similarity(timbre, "euclidean") %>%
  ggplot(aes(xstart + xduration*0.5, ystart + yduration*0.5, width=xduration, height=yduration, fill=d)) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option="mako", guide="none") +
  labs(title=title, x="Time (seconds)", y="", caption=caption) +
  theme_minimal() +
  # allow html in caption for coloring
  theme(plot.caption=ggtext::element_markdown()) +
  custom_theme
}

plot_grid(
  get_timbre_plot(
    valent_track, "Everybody's Got Something To Hide\nExcept Me And My Monkey\nby The Beatles\n(Lyrical valence = 0.96)",
    caption=sprintf(
    "<br>The <span style='color:%s'>%s</span> lines refer to unique sections,<br><span style='color:%s'>%s</span> or <span style='color:%s'>%s</span> lines to repeating sections<br>with the lyrical valence for that section",
    LINE_COLS[1], LINE_COLS[1], LINE_COLS[2], LINE_COLS[2], LINE_COLS[3], LINE_COLS[3])
  ) +
    ggplot2::annotate("segment", x=1, xend=8, y=1, yend=8, lineend="round", size=LINE_SIZE, col=LINE_COLS[1]) +
    ggplot2::annotate("segment", x=8, xend=37, y=8, yend=37, lineend="round", size=LINE_SIZE, col=LINE_COLS[2]) +
    ggplot2::annotate("segment", x=37, xend=75, y=37, yend=75, lineend="round", size=LINE_SIZE, col=LINE_COLS[3]) +
    ggplot2::annotate("segment", x=75, xend=104, y=75, yend=104, lineend="round", size=LINE_SIZE, col=LINE_COLS[2]) +
    ggplot2::annotate("segment", x=104, xend=125, y=104, yend=125, lineend="round", size=LINE_SIZE, col=LINE_COLS[3]) +
    ggplot2::annotate("segment", x=128, xend=131, y=128, yend=131, lineend="round", size=LINE_SIZE, col=LINE_COLS[1]) +
    ggplot2::annotate("segment", x=133, xend=142, y=133, yend=142, lineend="round", size=LINE_SIZE, col=LINE_COLS[2]) +
    
    ggplot2::annotate("label", x=10, y=-3, label="NA", fill="white") +
    ggplot2::annotate("label", x=33, y=20, label=0.97, fill="white") +
    ggplot2::annotate("label", x=63, y=50, label=0.96, fill="white") +
    ggplot2::annotate("label", x=93, y=80, label=0.93, fill="white") +
    ggplot2::annotate("label", x=123, y=110, label=0.82, fill="white") +
    ggplot2::annotate("label", x=134.5, y=122.5, label=0, fill="white") +
    ggplot2::annotate("label", x=148, y=135, label=0, fill="white"),
  get_timbre_plot(unvalent_track, "I'm so Tired by The Beatles\n\n\n(Lyrical valence = -0.85)", caption="<br><br>") +
    ggplot2::annotate("segment", x=2, xend=43, y=2, yend=43, lineend="round", size=LINE_SIZE, col=LINE_COLS[2]) +
    ggplot2::annotate("segment", x=45, xend=65, y=45, yend=65, lineend="round", size=LINE_SIZE, col=LINE_COLS[3]) +
    ggplot2::annotate("segment", x=67, xend=87, y=67, yend=87, lineend="round", size=LINE_SIZE, col=LINE_COLS[2]) +
    ggplot2::annotate("segment", x=89, xend=106, y=89, yend=106, lineend="round", size=LINE_SIZE, col=LINE_COLS[3]) +
    ggplot2::annotate("segment", x=106, xend=122, y=106, yend=122, lineend="round", size=LINE_SIZE, col=LINE_COLS[1]) +
    
    ggplot2::annotate("label", x=38, y=25, label=-0.86, fill="white") +
    ggplot2::annotate("label", x=63, y=50, label=-0.71, fill="white") +
    ggplot2::annotate("label", x=86, y=73, label=-0.94, fill="white") +
    ggplot2::annotate("label", x=110, y=97, label=-0.74, fill="white") +
    ggplot2::annotate("label", x=120, y=107, label=0.75, fill="white")
)
```

***

*Hypothesis: different timbres correlate with different lyrical valences.*

I should rephrase the question of finding the relationship between *instrumentation* and lyrical valence slightly to the relationship between *timbre* and lyrical valence. Which instruments are playing in a song might be relatively easy for a human to figure out, for a computer this is an almost insurmountable task. The closest thing we actually can measure is timbre, also known as the 'color' of the sound. This definition might seem vague, but it is: timbre means anything but pitch, duration or loudness (though sometimes loudness is included).

In order for us to measure how timbre relates to lyrical valence we will compare two tracks on opposite sides of the valence spectrum. In particular, I chose [*Everybody's Got Something To Hide Except Me And My Monkey*](https://www.musixmatch.com/lyrics/The-Beatles/Everybody-s-Got-Something-to-Hide-Except-Me-and-My-Monkey/translation/spanish) and [*I'm So Tired*](https://www.musixmatch.com/lyrics/The-Beatles/I-m-So-Tired-Esher-Demo) written by John Lennon, because they allow us to see how the same artist writes lyrics in relation to timbre. For both these songs, the timbre-based self-similarity matrix enables us to compare timbre between sections and see how lyrical valence, which you can read next to the lines, changes when timbre changes.

However, after investigating the matrices there do not seem to be any significant discrepancies between sections, except for the outros. But those seems to be a hoaxes, as upon inspection of the lyrics of the first track the outro is just a repetition of the valence-less words 'come-on'. As for the second track, it's simply a repetition of two lines that have been used in previous sections. Because these two lines, which are lyrically somewhat high in valence, were isolated and repeated in the last segment, the lyrical sentiment became positive. In light of this evidence it appears that there is no connection between timbre and lyrical valence. Maybe interesting patterns could be detected if more tracks were compared, as it stands none have been observed.

The next thing to try is rhythm.

### Rhythm

```{r word cloud, warning=FALSE}

# NOTE: for an annoying reason one 1 word cloud can be displayed

GROUP_COUNT <- 80
MIN_FREQ <- 0.0015

median_tempo <- median(corpus$tempo)

# load all words
lyrics_db <- jsonlite::fromJSON(LYRICS_FILE)[c("track", "lyrics")]
rownames(lyrics_db) <- lyrics_db$track

# lyrics of all the slow songs
slow_lyrics <- c()
# lyrics of all the fast songs
fast_lyrics <- c()

# gather lyrics
for (i in 1:nrow(corpus)) {
    row <- corpus[i,]
    lyrics <- lyrics_db[row$track,]$lyrics
    
    if (row$tempo < median_tempo) {
      slow_lyrics <- c(slow_lyrics, lyrics)
    } else {
      fast_lyrics <- c(fast_lyrics, lyrics)
    }
}

clean_text <- function(txt) {
  all_lyrics <- Corpus(VectorSource(txt))
  
  # clean text
  all_lyrics <- all_lyrics %>%
    tm_map(removeNumbers) %>%
    tm_map(removePunctuation) %>%
    tm_map(stripWhitespace) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(removeWords, stopwords("english"))
  
  matrix <- as.matrix(TermDocumentMatrix(all_lyrics))
  words <- sort(rowSums(matrix), decreasing=TRUE)
  # get relative frequencies
  word_freqs <- data.frame(word=names(words), freq=words / sum(words))
    return(word_freqs)
}

# clean lyrics
slow_word_freqs <- clean_text(paste(slow_lyrics, collapse=" "))
fast_word_freqs <- clean_text(paste(fast_lyrics, collapse=" "))

normalize <- function(x) (x - min(x)) / (max(x) - min(x))

# compute differences in frequencies
all_words <- merge(slow_word_freqs, fast_word_freqs, by="word", suffixes=c("_slow", "_fast"))
all_words$freq_slow <- normalize(pmax(all_words$freq_slow - all_words$freq_fast, 0))
all_words$freq_fast <- normalize(pmax(all_words$freq_fast - all_words$freq_slow, 0))

word_freqs <- rbind(
  all_words[c("word", "freq_slow")] %>%
    rename(freq=freq_slow) %>%
    arrange(desc(freq)) %>%
    head(n=GROUP_COUNT) %>%
    mutate(col = "red"),

  all_words[c("word", "freq_fast")] %>%
    rename(freq=freq_fast) %>%
    arrange(desc(freq)) %>%
    head(n=GROUP_COUNT) %>%
    mutate(col = "skyblue")
) %>% filter(freq >= MIN_FREQ)

word_freqs %>%
  wordcloud2(
    fontFamily=portfolio_font, shuffle=FALSE,
    color=word_freqs$col, size=1.0, widgetsize=c(750, 750)
  )

```

***

*Hypothesis: higher tempo songs tend to be more aggressive and slower songs more sensual.*

Let's put this one to the test. For this hypothesis we'll denote songs that have a lower BPM than the median (< `r median_tempo`BPM) as slow songs, and the remainder as fast songs (≥ `r median_tempo`BPM).

So far we've explored only the lyrical valency property, but not the lyrics themselves. We might gain some
new insights if we look at the lyrics directly, so let's try it. One of the most useful tools for visualizing
patterns in textual data is a so-called `word cloud`, which you can see to the side. 
The words in <span style="color:skyblue">blue</span> refer to words that occur very frequently in
fast songs relative to slow songs, and vice versa for the <span style="color:red">red</span> words.

Immediately we can see instances that prove the hypothesis. Slow song words include sensual words such as *number* (as in, someone's phone number), *kiss*, *boy* and *hot*. These are words we would expect to encounter in a love song. Though what stands out is that 
*love* is included in the fast songs. There are also some odd ones out like *bones*.
As for the fast tracks we also find what one would expect, e.g. aggressive words like *kill*, *gun* and *ill*. Also, very noticeably, we find numerous verbs and filler words. This makes sense in a track where the singer (or rapper) has to keep up the pace in a high BPM track, and it's easiest for the listener and artist to reuse many of the common verbs and filler words to keep the information stream somewhat limited.

Most of the data in this plot seems to **confirm** the hypothesis (though there are exceptions, like *love* among the fast tracks).

### AI: hyperparameter tuning

```{r tune xgboost}
#! build dataset

# NOTE: can take a long time to run, set to TRUE if it needs to be computed
if (RUN_XGBOOST_TUNING) {
  # make process deterministic
  set.seed(421)

  data <- corpus[c("danceability", "energy", "key", "loudness", "mode",
                   "tempo", "valence", "lyrical_sentiment"
  )]
  # columns to numeric, centering and scaling is not necessary for tree based models
  data$mode <- as.numeric(factor(data$mode))
  data$key <- as.numeric(factor(data$key))
  
  # split into train and test set
  data_split <- initial_split(data, strata=lyrical_sentiment)
  data_train <- training(data_split)
  data_test <- testing(data_split)
  # cross validation
  train_data_cv <- vfold_cv(data_train, strata=lyrical_sentiment, v=2)
  
  # build model
  xgb_specs <- boost_tree(
    mode="regression", engine="xgboost",
    trees=tune(), tree_depth=tune(),
    min_n=tune(), loss_reduction=tune(),
    sample_size=tune(), mtry=tune(),
    learn_rate=tune()
  )
  
  xgb_model <- workflow() %>%
    add_formula(lyrical_sentiment ~ .) %>%
    add_model(xgb_specs)
  
  # configure grid search to find optimal parameters
  xgb_grid <- grid_latin_hypercube(
    trees(), tree_depth(), min_n(), loss_reduction(),
    sample_size=sample_prop(),
    finalize(mtry(), data_train),
    learn_rate(),
    size=30
  )
  
  # find optimal parameters
  doParallel::registerDoParallel()
  
  xgb_params <- tune_grid(
    xgb_model,
    resamples=train_data_cv,
    grid=xgb_grid,
    control=control_grid(save_pred=TRUE)
  )
  
  best_params <- select_best(xgb_params, metric="rmse")
  best_params_rmse <- show_best(xgb_params, metric="rmse")[1,]$mean
}
```

```{r plot xgboost params}
ggplotly(xgb_params %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mtry:sample_size, mean) %>%
  pivot_longer(mtry:sample_size, values_to="value", names_to="parameter") %>%
  ggplot(aes(value, mean, color=parameter)) +
  geom_point(show.legend=FALSE) +
  facet_wrap(~parameter, scales="free_x") +
  labs(title="XGBoost errors for possible combinations of parameter values", x="Parameter value", y="Mean RMSE (root mean square error)") +
  custom_theme
)
```

***

*Research: are there some hidden relationships that have yet to be found?*

So far we have plotted numerous relationships between different variables and debunked or confirmed a number of hypotheses.
Though the reason I picked those visualizations is because there seemed to be potential for an interesting correlation, there
is still a chance there exist some totally unexpected patterns. Because these may escape a mere human like me, maybe the right
machine learning tool can pick them up. So that's what we'll be trying.

The technique we'll use is one of *the* most successful machine learning algorithms of the modern day, called extreme gradient boosting (XGBoosting). In essence, it's an ensemble of many small decision trees that boost each other to achieve superior results. To uncover secret relationships we will train the network, using regression, to predict the lyrical valency of a song based on a whole array of inputs that the `Spotify` API delivers (like mode, tempo, musical valence, etc.). Before we dive in and use it, some preparations need to be taken care of. First, the corpus is split into a train set (to train the model) and test set (to evaluate the model). Next up, for XGBoosting to work we need to list a number of hyperparameters. The idea is to tune these hyperparameters, by training the model for many different combinations of hyperparameters and evaluating each using cross validation. In the plot you can see how well each parameter value works: the lower RMSE, the better. We automatically pick the optimal combination of hyperparameters so we can properly train and evaluate the final model.

### AI: results

```{r plot xgboost importancies}
final_xgb_model <- finalize_workflow(xgb_model, best_params)

# train model
ggplotly(final_xgb_model %>%
  fit(data=data_train) %>%
  extract_fit_parsnip() %>%
  vi(method="permute", train=data_train, target="lyrical_sentiment", metric="rmse",
     pred_wrapper=function(obj, newdata)predict(obj, new_data=newdata)$.pred,
     nsim=100
  ) %>%
  vip(geom = "boxplot") +
  # why are x and y flipped in the plot?!
  labs(title="Feature importances for XGBoosting", x="Feature", y="Importance%")
)
```

```{r evaluate xgboost model}
xgb_rmse <- (collect_metrics(last_fit(final_xgb_model, data_split)) %>% filter(.metric == "rmse"))$.estimate

# compute RMSE for a random model
guesses <- runif(nrow(data_test))
random_rmse <- sqrt(mean((data_test$lyrical_sentiment - guesses)^2))
```


***

After training the model, we end up with a RMSE (root mean square error) of ``r xgb_rmse``. This means that the model, on average, is that
far off from the correct answer. As a reminder: the lyrical valency ranges from `-1` to `1`. It could well be that this still sounds
quite abstract. As a comparison I also evaluated a model that always makes a random guess. That model performs with a RMSE
of ``r random_rmse``, which is significantly worse. Therefore, the XGBoosting model must have found some pattern. That
makes it worth looking at whatever it found. One valuable piece of information that we can extract from the XGBoosting model, is the
set of feature importances it learned. They tell us how important each input parameter is deemed for predicting the lyrical valency, which you can see in the plot.

Many of the findings we found ourselves already. We already discovered that the mode of the key does not matter.
We found that musical valence and tempo correlate with lyrical valency. More interesting is what we did not find.
Apparently, according to the model, energy is the most predictive factor of the lyrical valency, even more so than musical valency.
This makes sense intuitively. High energy songs might be more likely to have more energetic lyrics. The model also judges
loudness and danceability as somewhat important features. A reason could be that the genre of a track defines in which range those features, including energy, belong, and that the genre also defines what the lyrics are generally about.

### Conclusion

After exploring the four elementary facets of music, i.e. melody, harmony, instrumentation and rhythm, we gained numerous insights on the question: how does music relate to the lyrics? We've seen some musical aspects that correlate with properties of lyrics and some that don't. On a high level we've learned that musical valence is a good predictor of lyrical valence. But can we figure out what low level features are at the root of this? Though melody lines are hard to quantify, we've looked at all 24 minor and major keys and found that surprisingly the mode actually does not make a difference. Among keys there seems to be some variance in lyrical valence, but this could be eliminated with an even bigger dataset. Furthermore, we've found no reason to suspect that instrumentation, measured by timbre, has an effect on lyrical valence or vice versa. Tempo, as a matter of fact, has a clear influence on lyrics. This could be explained by the fact that many subgenres stick to approximately the same tempo and deal with the same lyrical subjects, for example some parts of rap. Lastly, AI helped us to find patterns that we did not find ourselves, like that energy is highly predictive of lyrical valence. In summary, there are definitely relationships between lyrics and music, though not in all areas.

That being said, the last word hasn't been spoken on this subject. For future research it would be extremely interesting to delve deeper into more niche questions, like how Latin music lyrically relates to music from the USA. Dissecting the interconnectedness of lyrics and music may uncover countless and much deeper understandings of cultures across the globe. Furthermore, research like this attempts to discover how our feelings (music) connect with our intellect and analytical skills required for verbal processing (lyrics). Continued research will likely yield valuable knowledge about the human brain and what makes humans human.

